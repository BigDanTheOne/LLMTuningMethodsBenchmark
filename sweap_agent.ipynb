{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "213c1b45-f1a1-4afc-9534-b6c4ff7aa9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ed0582d-708e-46eb-8370-e9b80202a513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 09:10:02.914932: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-15 09:10:03.787728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling, EvalPrediction, BitsAndBytesConfig\n",
    "from transformers.trainer_pt_utils import nested_concat\n",
    "from datasets import load_dataset, load_metric\n",
    "import torch\n",
    "from pynvml import *\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import wandb\n",
    "from peft import LoraConfig, get_peft_model, IA3Config, PromptEncoderConfig, PrefixTuningConfig, AdaLoraConfig, prepare_model_for_kbit_training\n",
    "import numpy as np\n",
    "from torch.nn.functional import cross_entropy\n",
    "import transformers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "transformers.logging.set_verbosity_info()\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500a09b1-b406-458c-abca-67e5686eb76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mknowledge-ai-online\u001b[0m (\u001b[33mfuck-my-diploma\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_API_KEY\"]=\"\"\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"\"\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"true\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "242cade8-90b3-48d3-a6a4-d551ed0585e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5131a6fc-4df5-4d68-85f9-2a247e8c5fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 4634 MB.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "812120de-cbbf-404f-8195-5bc3396b5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_save_dataset(dataset_name='wikitext', dataset_config='wikitext-2-raw-v1', save_path='wikitext-2'):\n",
    "    # Load the dataset from Hugging Face\n",
    "    dataset = load_dataset(dataset_name,  dataset_config)\n",
    "\n",
    "    # Save the training data to a file\n",
    "    with open(save_path + '-train.txt', 'w', encoding='utf-8') as f:\n",
    "        for text in dataset['train']['text']:\n",
    "            f.write(text + '\\n')\n",
    "    with open(save_path + '-val.txt', 'w', encoding='utf-8') as f:\n",
    "        for text in dataset['validation']['text']:\n",
    "            f.write(text + '\\n')\n",
    "    with open(save_path + '-test.txt', 'w', encoding='utf-8') as f:\n",
    "        for text in dataset['test']['text']:\n",
    "            f.write(text + '\\n')\n",
    " \n",
    "\n",
    "# Download WikiText-2 and save it\n",
    "# download_and_save_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb8b257d-2447-4fb5-8e12-8156cf402d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_parameters(model, unfreeze_last_n=2):\n",
    "    # Freeze all parameters first\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the last `unfreeze_last_n` transformer blocks\n",
    "    for i in range(1, unfreeze_last_n + 1):\n",
    "        for param in model.transformer.h[-i].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Unfreeze the embedding layer parameters\n",
    "    for param in model.transformer.wte.parameters():  # Word token embeddings\n",
    "        param.requires_grad = True\n",
    "    for param in model.transformer.wpe.parameters():  # Positional embeddings\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Unfreeze the LM head\n",
    "    for param in model.lm_head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def training_step(self, model: torch.nn.Module, inputs: dict) -> torch.Tensor:\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        \n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        kwargs = {}\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        if self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            self.accelerator.backward(loss, **kwargs)\n",
    "\n",
    "        # Update and allocate with AdaLoRA\n",
    "        model.base_model.update_and_allocate(self.state.global_step)\n",
    "\n",
    "        return loss.detach() / self.args.gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc93247-0bef-47ef-b55d-4ab9c864e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    logits_list, labels_list = eval_pred\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "\n",
    "    for logits, labels in zip(logits_list, labels_list):\n",
    "        # Apply mask to exclude padding (or other ignore indices marked by -100)\n",
    "        mask = labels != -100\n",
    "        valid_logits = logits[mask]\n",
    "        valid_labels = labels[mask]\n",
    "\n",
    "        # Calculate loss for each batch and accumulate\n",
    "        if valid_labels.size > 0:  # Use .size for numpy arrays\n",
    "            # Reshape logits and labels appropriately\n",
    "            valid_logits = valid_logits.reshape(-1, valid_logits.shape[-1])\n",
    "            valid_labels = valid_labels.reshape(-1)\n",
    "\n",
    "            # Compute softmax on the logits to convert them to probabilities\n",
    "            exp_logits = np.exp(valid_logits - np.max(valid_logits, axis=1, keepdims=True))\n",
    "            softmax_logits = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "            # Compute cross entropy loss\n",
    "            log_probs = -np.log(softmax_logits[np.arange(valid_labels.size), valid_labels] + 1e-9)  # Add a small constant for numerical stability\n",
    "            batch_loss = np.mean(log_probs)\n",
    "\n",
    "            total_loss += batch_loss * valid_labels.size\n",
    "            total_count += valid_labels.size\n",
    "\n",
    "    # Calculate perplexity from total accumulated loss and count\n",
    "    if total_count > 0:\n",
    "        average_loss = total_loss / total_count\n",
    "        perplexity = np.exp(average_loss)\n",
    "    else:\n",
    "        perplexity = float('inf')  # Handle cases with no valid data\n",
    "\n",
    "    return {\"perplexity\": perplexity}\n",
    "\n",
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        # Prepare the model\n",
    "        model_name = 'gpt2'\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        if  \"qlora\" in config.peft_method:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=\"float16\",\n",
    "                bnb_4bit_use_double_quant=False,\n",
    "            )\n",
    "            model = GPT2LMHeadModel.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=bnb_config\n",
    "            )\n",
    "        else:\n",
    "            model = GPT2LMHeadModel.from_pretrained(\n",
    "                model_name\n",
    "            )\n",
    "    \n",
    "        if \"lora\" in config.peft_method:\n",
    "            peft_config = LoraConfig(\n",
    "                r=config.r,\n",
    "                lora_alpha=config.r * config.alpha_multiplier,\n",
    "                # target_modules=adapter_conf['target_modules'],\n",
    "                lora_dropout=config.lora_dropout,\n",
    "                fan_in_fan_out=config.fan_in_fan_out,\n",
    "                bias=config.bias,\n",
    "                task_type=\"CAUSAL_LM\"\n",
    "            )\n",
    "            model = get_peft_model(model, peft_config)\n",
    "        if \"rslora\" in config.peft_method:\n",
    "            peft_config = LoraConfig(\n",
    "                r=config.r,\n",
    "                lora_alpha=config.r * config.alpha_multiplier,\n",
    "                # target_modules=adapter_conf['target_modules'],\n",
    "                lora_dropout=config.lora_dropout,\n",
    "                fan_in_fan_out=config.fan_in_fan_out,\n",
    "                bias=config.bias,\n",
    "                use_rslora=config.use_rslora,\n",
    "                task_type=\"CAUSAL_LM\"\n",
    "            )\n",
    "            model = get_peft_model(model, peft_config)\n",
    "        if \"dora\" in config.peft_method:\n",
    "            peft_config = LoraConfig(\n",
    "                r=config.r,\n",
    "                lora_alpha=config.r * config.alpha_multiplier,\n",
    "                # target_modules=adapter_conf['target_modules'],\n",
    "                lora_dropout=config.lora_dropout,\n",
    "                fan_in_fan_out=config.fan_in_fan_out,\n",
    "                bias=config.bias,\n",
    "                use_dora=config.use_dora,\n",
    "                task_type=\"CAUSAL_LM\"\n",
    "            )\n",
    "            model = get_peft_model(model, peft_config)\n",
    "        elif \"adalora\" in config.peft_method:\n",
    "            peft_config = AdaLoraConfig(\n",
    "                init_r=config.init_r,\n",
    "                target_r=config.target_r,\n",
    "                lora_alpha = config.target_r * 2,\n",
    "                beta1=config.beta1,\n",
    "                beta2=config.beta2,\n",
    "                tinit=config.tinit,\n",
    "                total_step=config.total_step,\n",
    "                # lora_alpha=adapter_conf['lora_alpha'],\n",
    "                lora_dropout=0.01,\n",
    "                task_type=\"CAUSAL_LM\",\n",
    "                inference_mode=False,\n",
    "            )\n",
    "            model = get_peft_model(model, peft_config)\n",
    "        elif \"IA3\" in config.peft_method:\n",
    "            peft_config = IA3Config(\n",
    "                task_type=\"CAUSAL_LM\",\n",
    "                target_modules=config.config['target_modules'],\n",
    "                feedforward_modules=config.config['feedforward_modules']\n",
    "            )\n",
    "            model = get_peft_model(model, peft_config)\n",
    "        elif \"qlora\" in config.peft_method:\n",
    "            model.gradient_checkpointing_enable()\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "            peft_config = LoraConfig(\n",
    "                r=config.r,\n",
    "                lora_alpha=config.r * config.alpha_multiplier,\n",
    "                # target_modules=adapter_conf['target_modules'],\n",
    "                lora_dropout=config.lora_dropout,\n",
    "                fan_in_fan_out=config.fan_in_fan_out,\n",
    "                bias=config.bias,\n",
    "                task_type=\"CAUSAL_LM\"\n",
    "            )\n",
    "            model = get_peft_model(model, peft_config)\n",
    "\n",
    "        # if config.unfreeze_last_n != -1:\n",
    "        #     freeze_parameters(model, unfreeze_last_n=config.unfreeze_last_n)\n",
    "\n",
    "        \n",
    "        if \"prefix tuning\" in config.peft_method:\n",
    "            peft_config = PrefixTuningConfig(\n",
    "                num_virtual_tokens=config.num_virtual_tokens,\n",
    "                prefix_projection=config.prefix_projection,\n",
    "                task_type=\"CAUSAL_LM\"\n",
    "            )\n",
    "            model = get_peft_model(model, peft_config)\n",
    "        elif \"p-tuning\" in config.peft_method:\n",
    "            peft_config = PrefixTuningConfig(\n",
    "                num_virtual_tokens=config.num_virtual_tokens,\n",
    "                encoder_hidden_size=config.encoder_hidden_size,\n",
    "                task_type=\"CAUSAL_LM\"\n",
    "            )\n",
    "            model = get_peft_model(model, peft_config)\n",
    "\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'prompt' in name or 'lora' in name:\n",
    "                param.requires_grad = True\n",
    "\n",
    "            \n",
    "        train_dataset = TextDataset(\n",
    "            tokenizer=tokenizer,\n",
    "            file_path='wikitext-2-train.txt',\n",
    "            block_size=512)\n",
    "        val_dataset = TextDataset(\n",
    "            tokenizer=tokenizer,\n",
    "            file_path='wikitext-2-val.txt',\n",
    "            block_size=512)\n",
    "    \n",
    "        model = model.to(device)\n",
    "    \n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, mlm=False\n",
    "        )\n",
    "    \n",
    "        # Set training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='adalora_sweap',\n",
    "            report_to=\"wandb\",\n",
    "            # run_name=run_name, \n",
    "            # overwrite_output_dir=True,\n",
    "            num_train_epochs=5,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            save_steps=10_000,\n",
    "            # save_total_limit=2,\n",
    "            save_strategy='epoch',\n",
    "            evaluation_strategy='epoch',\n",
    "            logging_strategy='epoch',\n",
    "            # eval_accumulation_steps=4,\n",
    "            # dataloader_pin_memory=False,\n",
    "            include_tokens_per_second=True,\n",
    "            include_num_input_tokens_seen=True,\n",
    "            # eval_do_concat_batches=False,\n",
    "        )\n",
    "        \n",
    "        # Initialize the Trainer\n",
    "        if config.peft_method == \"adalora\":\n",
    "            # Initialize the CustomTrainer\n",
    "            trainer = CustomTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                data_collator=data_collator,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=val_dataset,\n",
    "                # compute_metrics=compute_metrics,\n",
    "            )\n",
    "        else:\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                data_collator=data_collator,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=val_dataset,\n",
    "                # compute_metrics=compute_metrics,\n",
    "            )\n",
    "    \n",
    "        print(f\"Training with config = {config}, \\n Number of trainable parametrs = {trainer.get_num_trainable_parameters()}\")\n",
    "        wandb.log({\"number_of_params\": trainer.get_num_trainable_parameters()}) \n",
    "        trainer.evaluate()\n",
    "        trainer.train()\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04de096-8c79-4699-b2b0-1c2123a1a1a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.agent('your_sweep_id', train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
